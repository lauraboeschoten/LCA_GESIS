[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Material Latent Class Analysis",
    "section": "",
    "text": "On this webpage, you can find all the course materials for the GESIS course “Latent Class Analysis”."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#day-1",
    "href": "index.html#day-1",
    "title": "Course Material Latent Class Analysis",
    "section": "Day 1",
    "text": "Day 1\n\n\n\nTopic\nLecture\nLab\nExtra material\n\n\n\n\nIntroduction\nLecture Introduction\n-\n-\n\n\nEM algorithm\nLecture EM\nLab EM\n-\n\n\nLCA basics\nLecture LCA basics\nLab LCA\nLCA_Excel Lab dataset"
  },
  {
    "objectID": "Labs/Day_1/Lab_EM/3_Lab_EM.html",
    "href": "Labs/Day_1/Lab_EM/3_Lab_EM.html",
    "title": "EM: simple example",
    "section": "",
    "text": "Below you will find code to simulate data from a mixture of univariate Gaussians, and estimate that mixture (LPA model) using the EM algorithm. The code is intended to be easy to understand and as simple as possible, while still doing the job.\nYou can copy code into your R environment by clicking the copy icon in the top right of each code block. You can also obtain the source code for this entire document by clicking “Code” at the top-right of this page.\n\nLook through the code and make the exercises. Your instructor is of course around to help and collaboration among participants is encouraged.\nBelow the regular questions, you will find a few “BONUS” questions. These are for those among you who are looking for a serious challenge, and you will likely find the difficulty level of these questions considerably higher. Do not worry if you do not understand these BONUS questions. Their completion is not needed for an applied understanding of LCA!\nQuestion 1\n\nset.seed(201505) # To reproduce the same \"random\" numbers\n# These are actual estimated values from the NHANES study.\n# We will take these as the true means and standard deviations \ntrue_mean_men &lt;- 1.74#m\ntrue_mean_women &lt;- 1.58#m\ntrue_sd_men &lt;- 0.08#m\ntrue_sd_women &lt;- 0.07#m\n\n# Generate fake data from the mixture distribution\nn &lt;- 1000 # Sample size\ntrue_sex &lt;- rbinom(n, size = 1, prob=0.5) + 1\n# Height is normally distributed but with different means and stdevs for men and women.\nheight &lt;- rnorm(n, c(true_mean_men, true_mean_women)[true_sex], \n                c(true_sd_men, true_sd_women)[true_sex])\n\n# Look at the data\nhist(height)\n\n\n\n\n\n\n\nRead the simulation code. Do you understand the code? Can you explain in your own words what happens here? Do you have any questions?\nQuestion 2\n\nrunit &lt;- function(maxit=3, sep_start=0.2) {\n  \n  # Choose some starting values. I choose inaccurate ones on purpose here\n  guess_mean_men &lt;- mean(height) + sep_start # We need to start with differences\n  guess_mean_wom &lt;- mean(height) - sep_start\n  guess_sd_men &lt;- sd(height)\n  guess_sd_wom &lt;- sd(height)\n  \n  cat(\"Iter:\\tM:\\tF:\\tsd(M):\\tsd(F):\\t\\n---------------------------------------\\n\")\n  cat(sprintf(\"Start\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", \n              guess_mean_men, guess_mean_wom, \n              guess_sd_men, guess_sd_wom))\n  \n  for(it in 1:maxit) {\n    # Posterior probability of being a man is the estimated proportion of the \n    #    overall height of the probability curve for men+women \n    #    that is made up by the probability curve for men:\n    pman &lt;- dnorm(height, mean = guess_mean_men, sd = guess_sd_men)\n    pwom &lt;- dnorm(height, mean = guess_mean_wom, sd = guess_sd_wom)\n    \n    post &lt;- pman / (pman + pwom)\n    \n    # The means and standard deviations for the groups of men and women are\n    #    obtained simply by using the posterior probabilities as weights. \n    # E.g. somebody with posterior 0.8 of being a man is counted 80% towards\n    #    the mean of men, and 20% towards that of women.\n    guess_mean_men &lt;- weighted.mean(height, w = post)\n    guess_mean_wom &lt;- weighted.mean(height, w = 1-post)\n    \n    guess_sd_men &lt;- sqrt(weighted.mean((height - guess_mean_men)^2, w = post))\n    guess_sd_wom &lt;- sqrt(weighted.mean((height - guess_mean_wom)^2, w = 1-post))\n    \n    # Output some of the results\n    cat(sprintf(\"%d\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", it,\n                guess_mean_men, guess_mean_wom, \n                guess_sd_men, guess_sd_wom))\n  }\n  \n  return(post) # Return the posterior probability of being a man\n}\n\nRead the function runit. Do you understand all steps?\nQuestion 3\nCode understanding check:\n\na. Why is the function dnorm used? Why is it used twice?\nb. What is happening here: post &lt;- pman / (pman + pwom)? What is the intuitive explanation of this formula?\nc. Why is weighted.mean used rather than just mean?\nd. Why are the weights chosen the way they are? e. What is the function of the for loop? When will it stop? Can you think of a different stopping rule?\n\nNow run the EM algorithm using our very own runit function.\n\n## Run the model!\n\n# Use height data to run the EM algorithm that estimates the means and stdevs of\n#    interest. Some intermediate output will be written to the screen.\npost &lt;- runit(maxit=5, sep_start=0.2)\n\nIter:   M:  F:  sd(M):  sd(F):  \n---------------------------------------\nStart   1.86    1.46    0.107   0.107\n1   1.74    1.58    0.072   0.066\n2   1.74    1.58    0.072   0.066\n3   1.74    1.58    0.073   0.066\n4   1.74    1.58    0.073   0.065\n5   1.74    1.58    0.073   0.065\n\n\nQuestion 4\nModel understanding check:\n\na. How does the EM algorithm know which group refers to men and which group refers to women? (Hint: this is a trick question)\nb. What is the height of the normal distribution curve (probability density) for: i. A 1.5 meter tall man ii. A 1.8 meter tall man iii. A 1.5 meter tall woman iv. A 1.8 meter tall woman\nc. From part (b), calculate the posterior probability to belong to the “women” class for: i. A 1.5 meter tall person of unknown sex (That is, calculate \\(P(\\text{Sex}=\\text{Woman} | \\text{Height} = 1.5)\\)) ii. A 1.8 meter tall person of unknown sex (same as above).\n\nQuestion 5\nGuessing people’s sex based on their posterior probability is not perfect. We can see this by making a cross-table between the guessed class and the true class (which here we happen to know because we created that variable ourselves in the simulation). So we guess the class based on the posterior probability and then tabulate this against the true class.\n\nsex &lt;- as.factor(true_sex)\n# Guess woman if posterior probability of being a man is less than 50%:\nguess_person_sex &lt;- as.factor(post &lt; 0.5) \nlevels(sex) &lt;- levels(guess_person_sex) &lt;- c(\"Man\", \"Woman\")\n\nknitr::kable(table(guess_person_sex, true_sex))\n\n\n\n\n1\n2\n\n\n\nMan\n436\n60\n\n\nWoman\n83\n421\n\n\n\n\n\nThis table gives the probability of being classified as a man/woman, given that you truly are one:\n\ntable(guess_person_sex, true_sex) |&gt;\n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\n\n1\n2\n\n\n\nMan\n0.8401\n0.1247\n\n\nWoman\n0.1599\n0.8753\n\n\n\n\n\nThis table is sometimes called the classification table. It is a measure of separation between the classes, and plays an important role when you want to use the classifications for some subsequent analysis. In practice, it cannot be calculated because we do not have the true_sex. Instead, an estimate can be calculated using the posterior probabilities. If these are well-calibrated (correspond to the true uncertainty about class membership), then calculating the within-guess mean of the posterior should give the desired classification table.\n\ncount_guesses &lt;- tabulate(guess_person_sex)\n\ntab &lt;- rbind(\n  tapply(post, guess_person_sex, mean), \n  tapply(1 - post, guess_person_sex, mean))\n\n# The table now estimates the probability of true class given guess. \n# We first recalculate this to a simple crosstable with counts.\nt(tab * count_guesses) |&gt; knitr::kable(digits = 1)\n\n\n\nMan\n439.5\n57.4\n\n\nWoman\n60.6\n442.4\n\n\n\n\nAgain we can show the table using column proportions, to give an estimate of the chance of correct classification given true class membership.\n\nt(tab * count_guesses) |&gt; \n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\nMan\n0.8788\n0.1148\n\n\nWoman\n0.1212\n0.8852\n\n\n\n\nQuestion 6\nBy changing the values of the relevant variables below, experiment with different settings for the means and standard deviations. Attempt to create a situation in which:\n\nThe cross-table between guessed class and true class is near-perfect;\nThe cross-table between guessed class and true class is near-useless.\n\nWhat do you conclude?\nQuestion 7\nChange the sample size to the following settings and report your findings: \\(n = 20, 50, 100, 500, 1000, 10000\\). (Be sure to re-set the parameter values for the means and standard deviations to their original values).\nQuestion 8\nUsing flexmix, we can run the same model.\n\n# Do the same as above with the flexmix library:\nlibrary(flexmix)\nheight_fit_flexmix &lt;- flexmix(height ~ 1, k = 2)\nparameters(height_fit_flexmix)\n\n                    Comp.1    Comp.2\ncoef.(Intercept) 1.6619860 1.6578146\nsigma            0.1078282 0.1070786\n\n\nSometimes flexmix converges to a local optimum. To solve this problem,we use multiple random starts (nrep = 100):\n\nheight_fit_flexmix &lt;- stepFlexmix(height ~ 1, k = 2, nrep = 100)\n\n2 : * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\nparameters(height_fit_flexmix)\n\n                     Comp.1     Comp.2\ncoef.(Intercept) 1.73242753 1.56980509\nsigma            0.07715975 0.06194694\n\n\nWe can also use mclust.\n\n# Or using the mclust library\nlibrary(mclust)\nheight_fit_mclust &lt;- Mclust(height)\nsummary(height_fit_mclust, parameters = TRUE)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust E (univariate, equal variance) model with 2 components: \n\n log-likelihood    n df      BIC      ICL\n       835.7496 1000  4 1643.868 1367.668\n\nClustering table:\n  1   2 \n542 458 \n\nMixing probabilities:\n        1         2 \n0.5365927 0.4634073 \n\nMeans:\n       1        2 \n1.583406 1.748475 \n\nVariances:\n          1           2 \n0.004763713 0.004763713 \n\n\nRun the code above to confirm that we can achieve the same result by using R packages.\nQuestion 9\nBONUS: In this exercise, we completely ignored the fact that the “prior” probabilities \\(\\pi = P(\\text{Sex} = \\text{Woman})\\) and \\(1-\\pi\\), which determine the class sizes, are not really known in practice. In other words, we set \\(\\pi\\) to its true value, \\(\\pi = 0.5\\) in our implementation. In practice, \\(\\pi\\) will be a parameter to be estimated. Implement code that does this. (Hint: you will need to adjust the E-step by using Bayes rule. The M-step for \\(\\pi\\) is just pi_est = mean(post).) Check your code by setting n to a large value, changing prob=0.5 in the simulation code to some substantially different number, and checking that your estimate corresponds to the true value. 8.\nQuestion 10 (BONUS)\nThe log-likelihood for this model is \\[\n    \\ell(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2 ; y) = \\sum_{i = 1}^n \\ln \\left[\n      \\pi \\cdot \\text{Normal}(\\mu_1, \\sigma_1) +\n      (1-\\pi) \\text{Normal}(\\mu_2, \\sigma_2)\n    \\right].\n    \\] Write code that calculates this log-likelihood, loglik, at each iteration of the EM for-loop. Double-check your code against the output of flexmix (or another package that provides this output). 9.\nQuestion 11 (BONUS)\nUsing the result from (9), implement code that terminates the for loop when the absolute relative decrease in log-likelihood, abs((loglik_current - loglik_previous)/loglik_current), say, is less than a tolerance value such as 0.001."
  },
  {
    "objectID": "Labs/Day_1/Lab_EM/3_Lab_EM.html#exercises",
    "href": "Labs/Day_1/Lab_EM/3_Lab_EM.html#exercises",
    "title": "EM: simple example",
    "section": "",
    "text": "Look through the code and make the exercises. Your instructor is of course around to help and collaboration among participants is encouraged.\nBelow the regular questions, you will find a few “BONUS” questions. These are for those among you who are looking for a serious challenge, and you will likely find the difficulty level of these questions considerably higher. Do not worry if you do not understand these BONUS questions. Their completion is not needed for an applied understanding of LCA!\nQuestion 1\n\nset.seed(201505) # To reproduce the same \"random\" numbers\n# These are actual estimated values from the NHANES study.\n# We will take these as the true means and standard deviations \ntrue_mean_men &lt;- 1.74#m\ntrue_mean_women &lt;- 1.58#m\ntrue_sd_men &lt;- 0.08#m\ntrue_sd_women &lt;- 0.07#m\n\n# Generate fake data from the mixture distribution\nn &lt;- 1000 # Sample size\ntrue_sex &lt;- rbinom(n, size = 1, prob=0.5) + 1\n# Height is normally distributed but with different means and stdevs for men and women.\nheight &lt;- rnorm(n, c(true_mean_men, true_mean_women)[true_sex], \n                c(true_sd_men, true_sd_women)[true_sex])\n\n# Look at the data\nhist(height)\n\n\n\n\n\n\n\nRead the simulation code. Do you understand the code? Can you explain in your own words what happens here? Do you have any questions?\nQuestion 2\n\nrunit &lt;- function(maxit=3, sep_start=0.2) {\n  \n  # Choose some starting values. I choose inaccurate ones on purpose here\n  guess_mean_men &lt;- mean(height) + sep_start # We need to start with differences\n  guess_mean_wom &lt;- mean(height) - sep_start\n  guess_sd_men &lt;- sd(height)\n  guess_sd_wom &lt;- sd(height)\n  \n  cat(\"Iter:\\tM:\\tF:\\tsd(M):\\tsd(F):\\t\\n---------------------------------------\\n\")\n  cat(sprintf(\"Start\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", \n              guess_mean_men, guess_mean_wom, \n              guess_sd_men, guess_sd_wom))\n  \n  for(it in 1:maxit) {\n    # Posterior probability of being a man is the estimated proportion of the \n    #    overall height of the probability curve for men+women \n    #    that is made up by the probability curve for men:\n    pman &lt;- dnorm(height, mean = guess_mean_men, sd = guess_sd_men)\n    pwom &lt;- dnorm(height, mean = guess_mean_wom, sd = guess_sd_wom)\n    \n    post &lt;- pman / (pman + pwom)\n    \n    # The means and standard deviations for the groups of men and women are\n    #    obtained simply by using the posterior probabilities as weights. \n    # E.g. somebody with posterior 0.8 of being a man is counted 80% towards\n    #    the mean of men, and 20% towards that of women.\n    guess_mean_men &lt;- weighted.mean(height, w = post)\n    guess_mean_wom &lt;- weighted.mean(height, w = 1-post)\n    \n    guess_sd_men &lt;- sqrt(weighted.mean((height - guess_mean_men)^2, w = post))\n    guess_sd_wom &lt;- sqrt(weighted.mean((height - guess_mean_wom)^2, w = 1-post))\n    \n    # Output some of the results\n    cat(sprintf(\"%d\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", it,\n                guess_mean_men, guess_mean_wom, \n                guess_sd_men, guess_sd_wom))\n  }\n  \n  return(post) # Return the posterior probability of being a man\n}\n\nRead the function runit. Do you understand all steps?\nQuestion 3\nCode understanding check:\n\na. Why is the function dnorm used? Why is it used twice?\nb. What is happening here: post &lt;- pman / (pman + pwom)? What is the intuitive explanation of this formula?\nc. Why is weighted.mean used rather than just mean?\nd. Why are the weights chosen the way they are? e. What is the function of the for loop? When will it stop? Can you think of a different stopping rule?\n\nNow run the EM algorithm using our very own runit function.\n\n## Run the model!\n\n# Use height data to run the EM algorithm that estimates the means and stdevs of\n#    interest. Some intermediate output will be written to the screen.\npost &lt;- runit(maxit=5, sep_start=0.2)\n\nIter:   M:  F:  sd(M):  sd(F):  \n---------------------------------------\nStart   1.86    1.46    0.107   0.107\n1   1.74    1.58    0.072   0.066\n2   1.74    1.58    0.072   0.066\n3   1.74    1.58    0.073   0.066\n4   1.74    1.58    0.073   0.065\n5   1.74    1.58    0.073   0.065\n\n\nQuestion 4\nModel understanding check:\n\na. How does the EM algorithm know which group refers to men and which group refers to women? (Hint: this is a trick question)\nb. What is the height of the normal distribution curve (probability density) for: i. A 1.5 meter tall man ii. A 1.8 meter tall man iii. A 1.5 meter tall woman iv. A 1.8 meter tall woman\nc. From part (b), calculate the posterior probability to belong to the “women” class for: i. A 1.5 meter tall person of unknown sex (That is, calculate \\(P(\\text{Sex}=\\text{Woman} | \\text{Height} = 1.5)\\)) ii. A 1.8 meter tall person of unknown sex (same as above).\n\nQuestion 5\nGuessing people’s sex based on their posterior probability is not perfect. We can see this by making a cross-table between the guessed class and the true class (which here we happen to know because we created that variable ourselves in the simulation). So we guess the class based on the posterior probability and then tabulate this against the true class.\n\nsex &lt;- as.factor(true_sex)\n# Guess woman if posterior probability of being a man is less than 50%:\nguess_person_sex &lt;- as.factor(post &lt; 0.5) \nlevels(sex) &lt;- levels(guess_person_sex) &lt;- c(\"Man\", \"Woman\")\n\nknitr::kable(table(guess_person_sex, true_sex))\n\n\n\n\n1\n2\n\n\n\nMan\n436\n60\n\n\nWoman\n83\n421\n\n\n\n\n\nThis table gives the probability of being classified as a man/woman, given that you truly are one:\n\ntable(guess_person_sex, true_sex) |&gt;\n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\n\n1\n2\n\n\n\nMan\n0.8401\n0.1247\n\n\nWoman\n0.1599\n0.8753\n\n\n\n\n\nThis table is sometimes called the classification table. It is a measure of separation between the classes, and plays an important role when you want to use the classifications for some subsequent analysis. In practice, it cannot be calculated because we do not have the true_sex. Instead, an estimate can be calculated using the posterior probabilities. If these are well-calibrated (correspond to the true uncertainty about class membership), then calculating the within-guess mean of the posterior should give the desired classification table.\n\ncount_guesses &lt;- tabulate(guess_person_sex)\n\ntab &lt;- rbind(\n  tapply(post, guess_person_sex, mean), \n  tapply(1 - post, guess_person_sex, mean))\n\n# The table now estimates the probability of true class given guess. \n# We first recalculate this to a simple crosstable with counts.\nt(tab * count_guesses) |&gt; knitr::kable(digits = 1)\n\n\n\nMan\n439.5\n57.4\n\n\nWoman\n60.6\n442.4\n\n\n\n\nAgain we can show the table using column proportions, to give an estimate of the chance of correct classification given true class membership.\n\nt(tab * count_guesses) |&gt; \n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\nMan\n0.8788\n0.1148\n\n\nWoman\n0.1212\n0.8852\n\n\n\n\nQuestion 6\nBy changing the values of the relevant variables below, experiment with different settings for the means and standard deviations. Attempt to create a situation in which:\n\nThe cross-table between guessed class and true class is near-perfect;\nThe cross-table between guessed class and true class is near-useless.\n\nWhat do you conclude?\nQuestion 7\nChange the sample size to the following settings and report your findings: \\(n = 20, 50, 100, 500, 1000, 10000\\). (Be sure to re-set the parameter values for the means and standard deviations to their original values).\nQuestion 8\nUsing flexmix, we can run the same model.\n\n# Do the same as above with the flexmix library:\nlibrary(flexmix)\nheight_fit_flexmix &lt;- flexmix(height ~ 1, k = 2)\nparameters(height_fit_flexmix)\n\n                    Comp.1    Comp.2\ncoef.(Intercept) 1.6619860 1.6578146\nsigma            0.1078282 0.1070786\n\n\nSometimes flexmix converges to a local optimum. To solve this problem,we use multiple random starts (nrep = 100):\n\nheight_fit_flexmix &lt;- stepFlexmix(height ~ 1, k = 2, nrep = 100)\n\n2 : * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\nparameters(height_fit_flexmix)\n\n                     Comp.1     Comp.2\ncoef.(Intercept) 1.73242753 1.56980509\nsigma            0.07715975 0.06194694\n\n\nWe can also use mclust.\n\n# Or using the mclust library\nlibrary(mclust)\nheight_fit_mclust &lt;- Mclust(height)\nsummary(height_fit_mclust, parameters = TRUE)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust E (univariate, equal variance) model with 2 components: \n\n log-likelihood    n df      BIC      ICL\n       835.7496 1000  4 1643.868 1367.668\n\nClustering table:\n  1   2 \n542 458 \n\nMixing probabilities:\n        1         2 \n0.5365927 0.4634073 \n\nMeans:\n       1        2 \n1.583406 1.748475 \n\nVariances:\n          1           2 \n0.004763713 0.004763713 \n\n\nRun the code above to confirm that we can achieve the same result by using R packages.\nQuestion 9\nBONUS: In this exercise, we completely ignored the fact that the “prior” probabilities \\(\\pi = P(\\text{Sex} = \\text{Woman})\\) and \\(1-\\pi\\), which determine the class sizes, are not really known in practice. In other words, we set \\(\\pi\\) to its true value, \\(\\pi = 0.5\\) in our implementation. In practice, \\(\\pi\\) will be a parameter to be estimated. Implement code that does this. (Hint: you will need to adjust the E-step by using Bayes rule. The M-step for \\(\\pi\\) is just pi_est = mean(post).) Check your code by setting n to a large value, changing prob=0.5 in the simulation code to some substantially different number, and checking that your estimate corresponds to the true value. 8.\nQuestion 10 (BONUS)\nThe log-likelihood for this model is \\[\n    \\ell(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2 ; y) = \\sum_{i = 1}^n \\ln \\left[\n      \\pi \\cdot \\text{Normal}(\\mu_1, \\sigma_1) +\n      (1-\\pi) \\text{Normal}(\\mu_2, \\sigma_2)\n    \\right].\n    \\] Write code that calculates this log-likelihood, loglik, at each iteration of the EM for-loop. Double-check your code against the output of flexmix (or another package that provides this output). 9.\nQuestion 11 (BONUS)\nUsing the result from (9), implement code that terminates the for loop when the absolute relative decrease in log-likelihood, abs((loglik_current - loglik_previous)/loglik_current), say, is less than a tolerance value such as 0.001."
  },
  {
    "objectID": "Labs/Day_1/Lab_LCA/5_Exercise.html",
    "href": "Labs/Day_1/Lab_LCA/5_Exercise.html",
    "title": "Exercise: Anti-religious speech",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(poLCA)\n\nRead the data from the General Social Survey 1987. It’s not old, it’s a classic!\n\nantireli &lt;- read.csv(\"https://daob.nl/files/lca/antireli_data.csv\")\n\nhead(antireli)\n\n  Y1 Y2 Y3\n1  1  1  1\n2  1  1  1\n3  1  1  1\n4  1  1  1\n5  1  1  1\n6  1  1  1\n\n\nShow the data as pattern frequencies.\n\ntable(antireli) |&gt; knitr::kable()\n\n\n\nY1\nY2\nY3\nFreq\n\n\n\n1\n1\n1\n696\n\n\n2\n1\n1\n34\n\n\n1\n2\n1\n275\n\n\n2\n2\n1\n125\n\n\n1\n1\n2\n68\n\n\n2\n1\n2\n19\n\n\n1\n2\n2\n130\n\n\n2\n2\n2\n366\n\n\n\n\n\nQuestion 1\nUse poLCA and fit a two-class LCA to these data.\n\nfit &lt;- poLCA(cbind(Y1, Y2, Y3) ~ 1, \n             data = antireli, \n             nclass = 2)\n\nConditional item response (column) probabilities,\n by outcome variable, for each class (row) \n \n$Y1\n           Pr(1)  Pr(2)\nclass 1:  0.9601 0.0399\nclass 2:  0.2284 0.7716\n\n$Y2\n           Pr(1)  Pr(2)\nclass 1:  0.7424 0.2576\nclass 2:  0.0429 0.9571\n\n$Y3\n           Pr(1)  Pr(2)\nclass 1:  0.9166 0.0834\nclass 2:  0.2395 0.7605\n\nEstimated class population shares \n 0.6205 0.3795 \n \nPredicted class memberships (by modal posterior prob.) \n 0.6264 0.3736 \n \n========================================================= \nFit for 2 latent classes: \n========================================================= \nnumber of observations: 1713 \nnumber of estimated parameters: 7 \nresidual degrees of freedom: 0 \nmaximum log-likelihood: -2795.376 \n \nAIC(2): 5604.751\nBIC(2): 5642.873\nG^2(2): 2.878816e-10 (Likelihood ratio/deviance statistic) \nX^2(2): 2.374666e-10 (Chi-square goodness of fit) \n \n\n\nQuestion 2\nCreate a profile plot.\n\nplot(fit)\n\n\n\n\n\n\n\nIn this case the default plot is still somewhat readable, but in practice it is not the best as data visualizations go. A simple line plot does a better job (in my personal & completely subjective opinion!) and allows you to display confidence intervals to boot. We use tidy from the broom package to extract the results and ggplot to plot.\n\ntidy(fit) %&gt;% \n  filter(outcome == 2) %&gt;% \n  mutate(class = as.factor(class)) %&gt;%\n  ggplot(aes(variable, estimate, group = class, color = class)) +\n  geom_point() + geom_line() + \n  geom_errorbar(aes(ymin = estimate - 2*std.error, \n                    ymax = estimate + 2*std.error), width = 0.2) +\n  theme_bw() +  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\nQuestion 3\nHow would you label the classes?\nQuestion 4\nWhat can you say about the estimated class sizes? What does this mean for the prevalence of the attitudes you labeled under (3)?\nQuestion 5\nModel fit\n\na. How many parameters are there?\nb. How many unique data patterns are there (fixing the sample size \\(n=1713\\))?\nc. Can you explain the number of degrees of freedom?\nd. Can you explain the value of the G^2 (\\(G^2\\)) and X^2 (\\(\\chi^2\\)) statistics?"
  },
  {
    "objectID": "Labs/Day_1/Lab_LCA/5_Exercise.html#exercises",
    "href": "Labs/Day_1/Lab_LCA/5_Exercise.html#exercises",
    "title": "Exercise: Anti-religious speech",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(poLCA)\n\nRead the data from the General Social Survey 1987. It’s not old, it’s a classic!\n\nantireli &lt;- read.csv(\"https://daob.nl/files/lca/antireli_data.csv\")\n\nhead(antireli)\n\n  Y1 Y2 Y3\n1  1  1  1\n2  1  1  1\n3  1  1  1\n4  1  1  1\n5  1  1  1\n6  1  1  1\n\n\nShow the data as pattern frequencies.\n\ntable(antireli) |&gt; knitr::kable()\n\n\n\nY1\nY2\nY3\nFreq\n\n\n\n1\n1\n1\n696\n\n\n2\n1\n1\n34\n\n\n1\n2\n1\n275\n\n\n2\n2\n1\n125\n\n\n1\n1\n2\n68\n\n\n2\n1\n2\n19\n\n\n1\n2\n2\n130\n\n\n2\n2\n2\n366\n\n\n\n\n\nQuestion 1\nUse poLCA and fit a two-class LCA to these data.\n\nfit &lt;- poLCA(cbind(Y1, Y2, Y3) ~ 1, \n             data = antireli, \n             nclass = 2)\n\nConditional item response (column) probabilities,\n by outcome variable, for each class (row) \n \n$Y1\n           Pr(1)  Pr(2)\nclass 1:  0.9601 0.0399\nclass 2:  0.2284 0.7716\n\n$Y2\n           Pr(1)  Pr(2)\nclass 1:  0.7424 0.2576\nclass 2:  0.0429 0.9571\n\n$Y3\n           Pr(1)  Pr(2)\nclass 1:  0.9166 0.0834\nclass 2:  0.2395 0.7605\n\nEstimated class population shares \n 0.6205 0.3795 \n \nPredicted class memberships (by modal posterior prob.) \n 0.6264 0.3736 \n \n========================================================= \nFit for 2 latent classes: \n========================================================= \nnumber of observations: 1713 \nnumber of estimated parameters: 7 \nresidual degrees of freedom: 0 \nmaximum log-likelihood: -2795.376 \n \nAIC(2): 5604.751\nBIC(2): 5642.873\nG^2(2): 2.878816e-10 (Likelihood ratio/deviance statistic) \nX^2(2): 2.374666e-10 (Chi-square goodness of fit) \n \n\n\nQuestion 2\nCreate a profile plot.\n\nplot(fit)\n\n\n\n\n\n\n\nIn this case the default plot is still somewhat readable, but in practice it is not the best as data visualizations go. A simple line plot does a better job (in my personal & completely subjective opinion!) and allows you to display confidence intervals to boot. We use tidy from the broom package to extract the results and ggplot to plot.\n\ntidy(fit) %&gt;% \n  filter(outcome == 2) %&gt;% \n  mutate(class = as.factor(class)) %&gt;%\n  ggplot(aes(variable, estimate, group = class, color = class)) +\n  geom_point() + geom_line() + \n  geom_errorbar(aes(ymin = estimate - 2*std.error, \n                    ymax = estimate + 2*std.error), width = 0.2) +\n  theme_bw() +  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\nQuestion 3\nHow would you label the classes?\nQuestion 4\nWhat can you say about the estimated class sizes? What does this mean for the prevalence of the attitudes you labeled under (3)?\nQuestion 5\nModel fit\n\na. How many parameters are there?\nb. How many unique data patterns are there (fixing the sample size \\(n=1713\\))?\nc. Can you explain the number of degrees of freedom?\nd. Can you explain the value of the G^2 (\\(G^2\\)) and X^2 (\\(\\chi^2\\)) statistics?"
  },
  {
    "objectID": "index.html#day-2",
    "href": "index.html#day-2",
    "title": "Course Material Latent Class Analysis",
    "section": "Day 2",
    "text": "Day 2\n\n\n\nTopic\nLecture\nLab\nExtra material\n\n\n\n\nModel fit\nLecture model fit\nLab model fit\n-\n\n\nClassification and covariates\nLecture classification and covariates\nLab classification and covariates\nExercise 1\n\n\nBayes rule and EM categorical\nLecture Bayes rule\nLecture EM categorical\nLab EM categorical\nExercise 2"
  },
  {
    "objectID": "Labs/Day_2/Lab_1/2_Lab.html",
    "href": "Labs/Day_2/Lab_1/2_Lab.html",
    "title": "Lab part 1: Political activism in Greece",
    "section": "",
    "text": "set.seed(202303)\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(haven)\nlibrary(poLCA)\n\n\nQuestion 1\nRead the data from the European Social Survey, round 4 (Greece).\nFor each of these survey questions, 1=“Yes” and 2=“No”.\n\n\ncontplt - Contacted politician or government official last 12 months\n\nwrkprty - Worked in political party or action group last 12 months\n\nwrkorg - Worked in another organisation or association last 12 months\n\nbadge - Worn or displayed campaign badge/sticker last 12 months\n\nsgnptit - Signed petition last 12 months\n\npbldmn - Taken part in lawful public demonstration last 12 months\n\nbctprd - Boycotted certain products last 12 months\n\ngndr - Gender\n\nagea - Age of respondent, calculated\n\n\ness_greece &lt;- read_csv(\"https://daob.nl/files/lca/ess_greece.csv.gz\") \n\ness_greece |&gt; rmarkdown::paged_table()\n\n\n  \n\n\n\nQuestion 2\nSadly, poLCA has no way of dealing with missing values other than “listwise deletion” (na.omit). For later comparability of models with different sets of variables, we create a single dataset without missings.\n\ness_greece &lt;- na.omit(ess_greece)\n\nQuestion 3\nWhat are the pattern frequencies of the data?\n\ntable(ess_greece) %&gt;% \n  as.data.frame() %&gt;%\n  filter(Freq != 0) %&gt;% \n  rmarkdown::paged_table()\n\n\n  \n\n\n\nQuestion 4\nCreate a convenience function that will fit the K-class model to the political participation data.\n\nfitLCA &lt;- function(k) {\n  f &lt;- cbind(contplt, wrkprty, wrkorg, badge, \n           sgnptit, pbldmn, bctprd) ~ 1\n  \n  poLCA(formula = f, data = ess_greece, nclass = k, \n        nrep = 10, verbose = FALSE)\n}\n\nQuestion 4\nApply the function to successively increasingly classes K = 1, 2, 3, …, 6. (Note: this can take a while!)\n\nMK &lt;- lapply(1:6, fitLCA)\n\nQuestion 5\nCompare the fit of the different models by looking at AIC, BIC, etc.\n\naic_values &lt;- sapply(MK, `[[`, \"aic\")\nbic_values &lt;- sapply(MK, `[[`, \"bic\")\n\n\nplot(seq_along(aic_values), aic_values, type = \"b\", xlab = \"Number of classes\", ylab = \"AIC\", las = 2)\n\n\n\n\n\n\n\n\nplot(seq_along(aic_values), aic_values, type = \"b\", xlab = \"Number of classes\", ylab = \"BIC\", las = 2)\n\n\n\n\n\n\n\nQuestion 5\nWhich model do you select? Print the profile of your selected model.\n\nform_activism &lt;- cbind(contplt, wrkprty, wrkorg, \n                       badge, sgnptit, pbldmn, bctprd) ~ 1\n\nfit &lt;- poLCA(form_activism, \n             data = ess_greece, \n             nclass = 4, \n             nrep = 20, verbose = FALSE)\n\nQuestion 6 Now plot the profile of your selected model. How would you substantively interpret the classes of this model?\nThis is the default plot given by polCA.\n\nplot(fit)\n\n\n\n\n\n\n\nIn this case the default plot is still somewhat readable, but in practice it is not the best as data visualizations go. A simple line plot does a better job (in my personal & completely subjective opinion!) and allows you to display confidence intervals to boot. We use tidy from the broom package to extract the results and ggplot to plot.\n\ntidy(fit) %&gt;% # from `broom` package\n  filter(outcome == 2) %&gt;% \n  mutate(class = as.factor(class)) %&gt;%\n  ggplot(aes(variable, estimate, group = class, color = class)) +\n  geom_point() + geom_line() + \n  geom_errorbar(aes(ymin = estimate - 2*std.error, \n                    ymax = estimate + 2*std.error), width = 0.2) +\n  theme_bw() + scale_color_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "Labs/Day_2/Lab_1/2_Lab.html#political-activism-in-ess",
    "href": "Labs/Day_2/Lab_1/2_Lab.html#political-activism-in-ess",
    "title": "Lab part 1: Political activism in Greece",
    "section": "",
    "text": "set.seed(202303)\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(haven)\nlibrary(poLCA)\n\n\nQuestion 1\nRead the data from the European Social Survey, round 4 (Greece).\nFor each of these survey questions, 1=“Yes” and 2=“No”.\n\n\ncontplt - Contacted politician or government official last 12 months\n\nwrkprty - Worked in political party or action group last 12 months\n\nwrkorg - Worked in another organisation or association last 12 months\n\nbadge - Worn or displayed campaign badge/sticker last 12 months\n\nsgnptit - Signed petition last 12 months\n\npbldmn - Taken part in lawful public demonstration last 12 months\n\nbctprd - Boycotted certain products last 12 months\n\ngndr - Gender\n\nagea - Age of respondent, calculated\n\n\ness_greece &lt;- read_csv(\"https://daob.nl/files/lca/ess_greece.csv.gz\") \n\ness_greece |&gt; rmarkdown::paged_table()\n\n\n  \n\n\n\nQuestion 2\nSadly, poLCA has no way of dealing with missing values other than “listwise deletion” (na.omit). For later comparability of models with different sets of variables, we create a single dataset without missings.\n\ness_greece &lt;- na.omit(ess_greece)\n\nQuestion 3\nWhat are the pattern frequencies of the data?\n\ntable(ess_greece) %&gt;% \n  as.data.frame() %&gt;%\n  filter(Freq != 0) %&gt;% \n  rmarkdown::paged_table()\n\n\n  \n\n\n\nQuestion 4\nCreate a convenience function that will fit the K-class model to the political participation data.\n\nfitLCA &lt;- function(k) {\n  f &lt;- cbind(contplt, wrkprty, wrkorg, badge, \n           sgnptit, pbldmn, bctprd) ~ 1\n  \n  poLCA(formula = f, data = ess_greece, nclass = k, \n        nrep = 10, verbose = FALSE)\n}\n\nQuestion 4\nApply the function to successively increasingly classes K = 1, 2, 3, …, 6. (Note: this can take a while!)\n\nMK &lt;- lapply(1:6, fitLCA)\n\nQuestion 5\nCompare the fit of the different models by looking at AIC, BIC, etc.\n\naic_values &lt;- sapply(MK, `[[`, \"aic\")\nbic_values &lt;- sapply(MK, `[[`, \"bic\")\n\n\nplot(seq_along(aic_values), aic_values, type = \"b\", xlab = \"Number of classes\", ylab = \"AIC\", las = 2)\n\n\n\n\n\n\n\n\nplot(seq_along(aic_values), aic_values, type = \"b\", xlab = \"Number of classes\", ylab = \"BIC\", las = 2)\n\n\n\n\n\n\n\nQuestion 5\nWhich model do you select? Print the profile of your selected model.\n\nform_activism &lt;- cbind(contplt, wrkprty, wrkorg, \n                       badge, sgnptit, pbldmn, bctprd) ~ 1\n\nfit &lt;- poLCA(form_activism, \n             data = ess_greece, \n             nclass = 4, \n             nrep = 20, verbose = FALSE)\n\nQuestion 6 Now plot the profile of your selected model. How would you substantively interpret the classes of this model?\nThis is the default plot given by polCA.\n\nplot(fit)\n\n\n\n\n\n\n\nIn this case the default plot is still somewhat readable, but in practice it is not the best as data visualizations go. A simple line plot does a better job (in my personal & completely subjective opinion!) and allows you to display confidence intervals to boot. We use tidy from the broom package to extract the results and ggplot to plot.\n\ntidy(fit) %&gt;% # from `broom` package\n  filter(outcome == 2) %&gt;% \n  mutate(class = as.factor(class)) %&gt;%\n  ggplot(aes(variable, estimate, group = class, color = class)) +\n  geom_point() + geom_line() + \n  geom_errorbar(aes(ymin = estimate - 2*std.error, \n                    ymax = estimate + 2*std.error), width = 0.2) +\n  theme_bw() + scale_color_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "Labs/Day_2/Lab_2/4_Lab.html",
    "href": "Labs/Day_2/Lab_2/4_Lab.html",
    "title": "Lab part 2: Political activisim in Greece",
    "section": "",
    "text": "Classification quality\nIn Lab 2, we continue with the model we created in Lab 1. You can continue in your previous script with the four class model, or run the following code:\n\nset.seed(202303)\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(haven)\nlibrary(poLCA)\nlibrary(ggplot2)\n\ness_greece &lt;- read_csv(\"https://daob.nl/files/lca/ess_greece.csv.gz\") \ness_greece &lt;- na.omit(ess_greece)\n\nform_activism &lt;- cbind(contplt, wrkprty, wrkorg, \n                       badge, sgnptit, pbldmn, bctprd) ~ 1\n\nfit &lt;- poLCA(form_activism, \n             data = ess_greece, \n             nclass = 4, \n             nrep = 20, verbose = FALSE)\n\nQuestion 1\nCreate a data frame with the posterior class memberships and predicted class has the actual classification (predclass is the “modal assignment”)\nUse the four-class model as the selected model\n\nposteriors &lt;- data.frame(post = fit$posterior,\n                         predclass = fit$predclass)\n\nclassification_table &lt;- posteriors %&gt;% \n  group_by(predclass) %&gt;% \n  summarize(across(starts_with(\"post.\"), ~ sum(.x)))\n\nclassification_table &lt;- classification_table[,-1] |&gt; as.matrix()\n\n# Adopt the notation X=true latent class, W=assigned class\ncolnames(classification_table) &lt;- paste0(\"X=\", 1:4)\nrownames(classification_table) &lt;- paste0(\"W=\", 1:4)\n\nclassification_table %&gt;% round(1)\n\n     X=1  X=2  X=3    X=4\nW=1 60.1  1.4  8.6    4.0\nW=2  0.2 19.8  1.0    0.0\nW=3  3.0  1.1 87.4    7.5\nW=4 11.1  0.0 34.8 1822.0\n\n\nWith column proportions:\n\nclassification_table |&gt;\n  prop.table(2) |&gt; \n  round(3)\n\n      X=1   X=2   X=3   X=4\nW=1 0.808 0.062 0.065 0.002\nW=2 0.003 0.887 0.008 0.000\nW=3 0.040 0.051 0.663 0.004\nW=4 0.149 0.000 0.264 0.994\n\n\nQuestion 2\nCalculate classification errors from classification table.\n\n1 - sum(diag(classification_table)) / sum(classification_table)\n\n[1] 0.03524704\n\n\nQuestion 3\nAnd now calculate the Entropy \\(R^2\\).\n\nentropy &lt;- function(p) sum(-p * log(p))\n\nerror_prior &lt;- entropy(fit$P) # Class proportions\nerror_post &lt;- mean(apply(fit$posterior, 1, entropy))\n(R2_entropy  &lt;- (error_prior - error_post) / error_prior) # 0.741\n\n[1] 0.7410987\n\n\nIncluding covariates\nQuestion 4\nNow fit the four-class model, but include covariates that predict the class membership. Class membership is predicted by gender and a quadratic age effect.\nWe also use the results from the model without covariates as starting values for the solution.\nThis is where the analyzed data would have been different if we had not already deleted all cases with at least one missing value above using na.omit. In practice this may lead to trouble, especially when there are many variables.\n\nform_activism &lt;- cbind(contplt, wrkprty, wrkorg, \n                       badge, sgnptit, pbldmn, bctprd) ~ \n  gndr + agea + I(agea^2)\n\ness_greece_poly &lt;- ess_greece %&gt;% \n  mutate(agea = scale(agea))\n\nfit_covariates &lt;-  \n  poLCA(form_activism, \n        data = ess_greece_poly, nclass = 4, \n        probs.start = fit$probs, \n        verbose = FALSE, nrep = 50, maxiter = 3e3)\n\nQuestion 5\nConfirm that the results now include a multinomial regression coefficients in a model predicting class membership.\n\nfit_covariates\n\nConditional item response (column) probabilities,\n by outcome variable, for each class (row) \n \n$contplt\n           Pr(1)  Pr(2)\nclass 1:  0.2460 0.7540\nclass 2:  0.7566 0.2434\nclass 3:  0.7619 0.2381\nclass 4:  0.0514 0.9486\n\n$wrkprty\n           Pr(1)  Pr(2)\nclass 1:  0.1066 0.8934\nclass 2:  0.9426 0.0574\nclass 3:  0.5423 0.4577\nclass 4:  0.0004 0.9996\n\n$wrkorg\n           Pr(1)  Pr(2)\nclass 1:  0.2283 0.7717\nclass 2:  1.0000 0.0000\nclass 3:  0.1618 0.8382\nclass 4:  0.0062 0.9938\n\n$badge\n           Pr(1)  Pr(2)\nclass 1:  0.1008 0.8992\nclass 2:  0.8711 0.1289\nclass 3:  0.3687 0.6313\nclass 4:  0.0000 1.0000\n\n$sgnptit\n           Pr(1)  Pr(2)\nclass 1:  0.5029 0.4971\nclass 2:  0.8243 0.1757\nclass 3:  0.0027 0.9973\nclass 4:  0.0056 0.9944\n\n$pbldmn\n           Pr(1)  Pr(2)\nclass 1:  0.3951 0.6049\nclass 2:  0.8277 0.1723\nclass 3:  0.2251 0.7749\nclass 4:  0.0158 0.9842\n\n$bctprd\n           Pr(1)  Pr(2)\nclass 1:  0.7298 0.2702\nclass 2:  0.6846 0.3154\nclass 3:  0.2121 0.7879\nclass 4:  0.0983 0.9017\n\nEstimated class population shares \n 0.0653 0.0117 0.0387 0.8843 \n \nPredicted class memberships (by modal posterior prob.) \n 0.049 0.0116 0.033 0.9064 \n \n========================================================= \nFit for 4 latent classes: \n========================================================= \n2 / 1 \n            Coefficient  Std. error  t value  Pr(&gt;|t|)\n(Intercept)    -1.20234     0.88172   -1.364     0.176\ngndr           -0.45999     0.55026   -0.836     0.405\nagea           -0.40722     0.48011   -0.848     0.399\nI(agea^2)      -0.00799     0.42691   -0.019     0.985\n========================================================= \n3 / 1 \n            Coefficient  Std. error  t value  Pr(&gt;|t|)\n(Intercept)    -0.24986     0.68744   -0.363     0.717\ngndr           -0.30756     0.40474   -0.760     0.449\nagea            0.51671     0.23895    2.162     0.033\nI(agea^2)       0.22320     0.19886    1.122     0.265\n========================================================= \n4 / 1 \n            Coefficient  Std. error  t value  Pr(&gt;|t|)\n(Intercept)     1.97587     0.40630    4.863     0.000\ngndr            0.26953     0.24745    1.089     0.279\nagea            0.27742     0.14518    1.911     0.059\nI(agea^2)       0.29597     0.13349    2.217     0.029\n========================================================= \nnumber of observations: 2062 \nnumber of estimated parameters: 40 \nresidual degrees of freedom: 87 \nmaximum log-likelihood: -2764.565 \n \nAIC(4): 5609.129\nBIC(4): 5834.387\nX^2(4): 126.1331 (Chi-square goodness of fit) \n \nALERT: estimation algorithm automatically restarted with new initial values \n \n\n\nQuestion 6\nCheck if the solution has changed now that covariates are included.\n\ntidy(fit_covariates) %&gt;% \n  filter(outcome == 2) %&gt;% \n  mutate(class = as.factor(class)) %&gt;%\n  ggplot(aes(variable, estimate, group = class, color = class)) +\n  geom_point() + geom_line() + \n  geom_errorbar(aes(ymin = estimate - 2*std.error, \n                    ymax = estimate + 2*std.error), width = 0.2) +\n  theme_bw() + scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\nQuestion 7\nPlot the results of the multinomial model\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Extract posterior probabilities of each class\nposterior_df &lt;- as.data.frame(fit_covariates$posterior)\n\n# Add covariates to the dataset\nposterior_df &lt;- ess_greece_poly %&gt;%\n  dplyr::select(gndr, agea) %&gt;%\n  bind_cols(posterior_df)\n\n# Rename class probability columns\ncolnames(posterior_df)[3:ncol(posterior_df)] &lt;- paste0(\"Class_\", 1:4)\n\n# Convert gender to factor for plotting\nposterior_df$gndr &lt;- as.factor(posterior_df$gndr)\n\n\n# Reshape data to long format for ggplot\nposterior_long &lt;- posterior_df %&gt;%\n  pivot_longer(cols = starts_with(\"Class_\"), \n               names_to = \"Class\", values_to = \"Probability\")\n\n# Plot probability of class membership as a function of age\nggplot(posterior_long, aes(x = agea, y = Probability, color = Class)) +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  labs(title = \"Effect of Age on Class Membership\",\n       x = \"Age\", y = \"Probability of Class Membership\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nggplot(posterior_long, aes(x = gndr, y = Probability, fill = Class)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", position = \"dodge\") +\n  labs(title = \"Effect of Gender on Class Membership\",\n       x = \"Gender\", y = \"Mean Probability of Class Membership\") +\n  theme_minimal()"
  },
  {
    "objectID": "Labs/Day_2/Lab_3/8_Lab.html",
    "href": "Labs/Day_2/Lab_3/8_Lab.html",
    "title": "EM: categorical indicators",
    "section": "",
    "text": "Below you will find code to simulate data from a mixture of conditionally independent binary indicators, and to estimate that mixture (LCA model) using the EM algorithm. The code is intended to be easy to understand and as simple as possible, while still doing the job. You can copy code into your R environment by clicking the copy icon in the top right of each code block."
  },
  {
    "objectID": "Labs/Day_2/Lab_3/8_Lab.html#em-for-categorical-indicators",
    "href": "Labs/Day_2/Lab_3/8_Lab.html#em-for-categorical-indicators",
    "title": "EM: categorical indicators",
    "section": "",
    "text": "Below you will find code to simulate data from a mixture of conditionally independent binary indicators, and to estimate that mixture (LCA model) using the EM algorithm. The code is intended to be easy to understand and as simple as possible, while still doing the job. You can copy code into your R environment by clicking the copy icon in the top right of each code block."
  },
  {
    "objectID": "Labs/Day_2/Lab_3/8_Lab.html#exercises",
    "href": "Labs/Day_2/Lab_3/8_Lab.html#exercises",
    "title": "EM: categorical indicators",
    "section": "Exercises",
    "text": "Exercises\nQuestion 1\n\nset.seed(202303)\n\nn &lt;- 1000L # Sample size\n\nP_X_true &lt;- 0.4 # True pi (will be class size of X=2)\n\n# Sample class memberships:\nX &lt;- sample(1:2, size = n, replace = TRUE, \n            prob = c(1 - P_X_true, P_X_true))\n\n# True profiles:\nP_Y.X_true &lt;- list(\n  Y1 = matrix(c(0.9, 0.2,\n                0.1, 0.8), byrow = TRUE, nrow = 2),\n  Y2 = matrix(c(0.7, 0.2,\n                0.3, 0.8), byrow = TRUE, nrow = 2),\n  Y3 = matrix(c(0.95, 0.4,\n                0.05, 0.6), byrow = TRUE, nrow = 2)\n)\n\nThe conditional (profile) probabilities \\(P(Y_j | X)\\) are:\n\nprint(P_Y.X_true)\n\n$Y1\n     [,1] [,2]\n[1,]  0.9  0.2\n[2,]  0.1  0.8\n\n$Y2\n     [,1] [,2]\n[1,]  0.7  0.2\n[2,]  0.3  0.8\n\n$Y3\n     [,1] [,2]\n[1,] 0.95  0.4\n[2,] 0.05  0.6\n\n\nWe now sample some data using the conditional probabilities and the values of X.\n\n# Sample observed indicators from binomials (Bernoullis):\nY1 &lt;- rbinom(n, size = 1, prob = P_Y.X_true[[1]][2, X])\nY2 &lt;- rbinom(n, size = 1, prob = P_Y.X_true[[2]][2, X])\nY3 &lt;- rbinom(n, size = 1, prob = P_Y.X_true[[3]][2, X])\n\ndf_samp &lt;- data.frame(Y1, Y2, Y3) # For other analyses\n\nRead the simulation code above. Can you explain in your own words what happens here? Do you have any questions about it?\nQuestion 2\nWe will take as parameters the probabilities of a “1” response on each of the three indicators, given \\(X=1\\) or \\(X=2\\), respectively. And of course the class size \\(\\pi = P(X = 2)\\). So there are 7 parameters in total. Since there are \\(2^3 = 8\\) observeed patterns, but only 7 independent ones, the degrees of freedom for this model equals zero.\n\n# As usual, we start by guessing parameter values\nguess_PY.X &lt;- list(\n  Y1 = c(0.4, 0.6),\n  Y2 = c(0.4, 0.6),\n  Y3 = c(0.4, 0.6)\n)\n# We will take PX (pi) to be P(X = 2)\nguess_PX &lt;- 0.5\n\n# Number of EM iterations\nmaxiter &lt;- 15\n\n# Start the EM algorithm!\nfor(it in 1:maxiter) {\n  # Just some output \n  if(it == 1) # A trick to make Quarto output this line correctly\n    cat(\"It:\\t(X=2)\\tY1|X=1\\tY1|X=2\\tY2|X=1\\tY2|X=2\\tY3|X=1\\tY3|X=2\\n\")\n  cat(sprintf(\"%03d\\t%1.3f\\t%1.3f\\t%1.3f\\t%1.3f\\t%1.3f\\t%1.3f\\t%1.3f\\n\", it,\n      guess_PX, \n      guess_PY.X$Y1[1], guess_PY.X$Y1[2],\n      guess_PY.X$Y2[1], guess_PY.X$Y2[2],\n      guess_PY.X$Y3[1], guess_PY.X$Y3[2]))\n\n  # E-step\n  # ------------------\n  # For clarity purposes I am not using a loop over the three variables. \n  #  In practice, you will probably want to do that. \n\n  # The probability of observing that value if X were X=1 or X=2\n  \n  # Here we use the assumption that each value is ~ Bernoulli\n  #. In practice you would work with logs, but here we ignore that\n  P_Y1.X1 &lt;- dbinom(Y1, size = 1, prob = guess_PY.X$Y1[1])\n  P_Y1.X2 &lt;- dbinom(Y1, size = 1, prob = guess_PY.X$Y1[2])\n  \n  P_Y2.X1 &lt;- dbinom(Y2, size = 1, prob = guess_PY.X$Y2[1])\n  P_Y2.X2 &lt;- dbinom(Y2, size = 1, prob = guess_PY.X$Y2[2])\n  \n  P_Y3.X1 &lt;- dbinom(Y3, size = 1, prob = guess_PY.X$Y3[1])\n  P_Y3.X2 &lt;- dbinom(Y3, size = 1, prob = guess_PY.X$Y3[2])\n  \n  # Now we use the conditional independence assumption \n  #.  to get the probability of the whole pattern (df_samp[i, ])\n  # (In practice you will want to takes a sum of logs instead)\n  P_Y_X1 &lt;-  P_Y1.X1 * P_Y2.X1 * P_Y3.X1\n  P_Y_X2 &lt;-  P_Y1.X2 * P_Y2.X2 * P_Y3.X2\n  \n  # Now we use the mixture assumption to get the marginal probability of the pattern:\n  P_Y &lt;- (1 - guess_PX)*P_Y_X1 + guess_PX*P_Y_X2\n  \n  # Finally we are ready to apply Bayes rule to get the posterior \n  #. P(X = 2 | Y = y)\n  post_X2 &lt;- guess_PX*P_Y_X2 / P_Y\n  \n  # M-step \n  # ------------------\n  # Now we have the posterior it is easy to calculate the probabilities we need\n  \n  # M-step for 'priors' / class size of X=2\n  guess_PX &lt;- mean(post_X2)\n  \n  # M-step for profiles\n  guess_PY.X$Y1[1] &lt;- weighted.mean(Y1, w = (1 - post_X2))\n  guess_PY.X$Y1[2] &lt;- weighted.mean(Y1, w = post_X2)\n  \n  guess_PY.X$Y2[1] &lt;- weighted.mean(Y2, w = (1 - post_X2))\n  guess_PY.X$Y2[2] &lt;- weighted.mean(Y2, w = post_X2)\n  \n  guess_PY.X$Y3[1] &lt;- weighted.mean(Y3, w = (1 - post_X2))\n  guess_PY.X$Y3[2] &lt;- weighted.mean(Y3, w = post_X2)\n}\n\nIt: (X=2)   Y1|X=1  Y1|X=2  Y2|X=1  Y2|X=2  Y3|X=1  Y3|X=2\n001 0.500   0.400   0.600   0.400   0.600   0.400   0.600\n002 0.426   0.228   0.538   0.341   0.647   0.145   0.420\n003 0.423   0.167   0.623   0.289   0.720   0.092   0.495\n004 0.415   0.125   0.692   0.256   0.775   0.057   0.552\n005 0.406   0.106   0.732   0.247   0.798   0.042   0.584\n006 0.399   0.099   0.753   0.250   0.804   0.037   0.601\n007 0.393   0.098   0.765   0.255   0.804   0.036   0.611\n008 0.389   0.098   0.773   0.259   0.804   0.035   0.618\n009 0.385   0.099   0.778   0.262   0.805   0.036   0.624\n010 0.381   0.100   0.782   0.264   0.806   0.037   0.628\n011 0.378   0.101   0.785   0.266   0.808   0.037   0.631\n012 0.376   0.102   0.788   0.267   0.810   0.038   0.633\n013 0.374   0.103   0.790   0.268   0.811   0.039   0.636\n014 0.371   0.105   0.792   0.269   0.813   0.040   0.638\n015 0.370   0.106   0.794   0.270   0.814   0.041   0.640\n\n\nRead the EM loop. Do you understand all steps?\nQuestion 3\n\nguess_PY.X |&gt; \n  lapply(function(x) rbind(1-x, x)) |&gt;\n  print(digits = 3)\n\n$Y1\n   [,1]  [,2]\n  0.894 0.204\nx 0.106 0.796\n\n$Y2\n  [,1]  [,2]\n  0.73 0.184\nx 0.27 0.816\n\n$Y3\n    [,1]  [,2]\n  0.9588 0.359\nx 0.0412 0.641\n\n\nInterpret the resulting estimates of the conditional probabilities (profiles) as printed by the code above.\nQuestion 4\nCompare these to the true profiles, which were:\n\nP_Y.X_true\n\n$Y1\n     [,1] [,2]\n[1,]  0.9  0.2\n[2,]  0.1  0.8\n\n$Y2\n     [,1] [,2]\n[1,]  0.7  0.2\n[2,]  0.3  0.8\n\n$Y3\n     [,1] [,2]\n[1,] 0.95  0.4\n[2,] 0.05  0.6\n\n\nQuestion 5\nThe estimated class sizes are\n\nc(1 - guess_PX, guess_PX) |&gt; \n  print(digits = 3)\n\n[1] 0.632 0.368\n\n\nCompare these to the “priors” (class sizes), which were:\n\nc(1 - P_X_true, P_X_true) |&gt; \n  print(digits = 3)\n\n[1] 0.6 0.4\n\n\nQuestion 6\nCode understanding check:\n\na. In the simulation code, explain why the subscripts 1 and 2 are used respectively in P_Y.X_true[[1]][2, X].\nb. Which values would you change if you wanted to implement random starts?\nc. Suppose the model said that the variables do not come from a binomial (Bernoulli) distribution, but from some other distribution (for example a Beta one). Which lines would you need to change?\n\nQuestion 7\nTry reversing the starting values, so, Y1 = c(0.4, 0.6) becomes Y1 = c(0.6, 0.4), and similarly for the other two variables. What happens to the estimates? How do these compare to the true values now?\nQuestion 8\nSet the number of iterations of the EM algorithm to a large number, such as maxiter = 200. What happens to the estimates?\nQuestion 9\nTry out different values of the prior, the profile probabilities, and the sample size. Report any interesting observations.\nQuestion 10\nFit the same model using poLCA (or otherwise). (Remember that you can directly analyze df_samp.) Do the results agree with our own EM implementation?\nQuestion 11\nBONUS: Calculate the log-likelihood of the model.\nQuestion 12\nBONUS: Investigate the entropy \\(R^2\\) of the posterior classification as a function of (a) the profile probabilities and (b) prior."
  },
  {
    "objectID": "Extra/Day_2/Exercise_2/9_Exercise.html",
    "href": "Extra/Day_2/Exercise_2/9_Exercise.html",
    "title": "PROJECT - Reproducing a published LCA",
    "section": "",
    "text": "In this project, you will work on reading and reproducing the following paper:\n  Beller, J. (2021). Morbidity profiles in Europe and Israel: International comparisons\n          from 20 countries using biopsychosocial indicators of health via latent class analysis.\n          Journal of Public Health. https://doi.org/10.1007/s10389-021-01673-0\nYou can download a copy of the paper here: https://daob.nl/files/lca/beller-2021.pdf\nThe data used in this study were from the European Social Survey, round 7 (2014). You can find these (and more recent) data here: https://www.europeansocialsurvey.org/data/. You will need to register to download the data. Registration is free and should be instantaneous."
  },
  {
    "objectID": "Extra/Day_2/Exercise_2/9_Exercise.html#reproducing-a-published-lca",
    "href": "Extra/Day_2/Exercise_2/9_Exercise.html#reproducing-a-published-lca",
    "title": "PROJECT - Reproducing a published LCA",
    "section": "",
    "text": "In this project, you will work on reading and reproducing the following paper:\n  Beller, J. (2021). Morbidity profiles in Europe and Israel: International comparisons\n          from 20 countries using biopsychosocial indicators of health via latent class analysis.\n          Journal of Public Health. https://doi.org/10.1007/s10389-021-01673-0\nYou can download a copy of the paper here: https://daob.nl/files/lca/beller-2021.pdf\nThe data used in this study were from the European Social Survey, round 7 (2014). You can find these (and more recent) data here: https://www.europeansocialsurvey.org/data/. You will need to register to download the data. Registration is free and should be instantaneous."
  },
  {
    "objectID": "Extra/Day_2/Exercise_2/9_Exercise.html#assignment",
    "href": "Extra/Day_2/Exercise_2/9_Exercise.html#assignment",
    "title": "PROJECT - Reproducing a published LCA",
    "section": "Assignment",
    "text": "Assignment\nQuestion 1\nObtain the data\nQuestion 2\nCreate the dataset as indicated in the article. You should end up with 16 indicators, (excluding any covariates such as country, age, gender, or education)\n\na. Which steps taken in the paper could you criticize? Do you think they will have a large impact on the final conclusions?\nb. Create two different versions of the data, one of which is as close as possible to the paper, the other differing only on the aspect of data wrangling that you believe will be the most influential.\nc. Create a smaller dataset, selecting only one country of your choice. You will use this smaller dataset to initially debug the subsequent analyses. From the list of covariates in table 4, select a maximum of two or three that you find of interest.\nd. For each recode, double-check your recode by creating a before/after cross-table, or by calculating means within categories for dichotomized variables. Did everything go according to your plan?\ne. Perform a sanity check on your data. Pay attention to Roger Peng’s EDA advice:\ni. Read in your data : Are the names of the variables correct, concise, and unambiguous?\nii. Check the packaging : Are the number of rows and columns as expected? Are the types of your variables as expected? Do your variables contain weird values (such as -99)? Are some variables all-missing or constant?\niii. Look at the top and the bottom of your data : use head() and tail() or some other method to check the top and bottom.\niv. Check your “n”s : do the sample sizes correspond to the documentation? To the paper?\nv. Validate with at least one external data source : check descriptives of your data against those in the paper (small differences may occur). Are distributions of study variables (e.g. depression) and background variables (e.g. age, gender, education) plausible? (you may want to look at official statistics for your chosen country)\nvi. Make a plot, look at descriptives : you are free to make sensible choices of “checks” here. Examples could include checking that correlations between closely related variables are not negative, scatterplots of continuous variables or dotplots/boxplots of continuous/categorical variabels to check for outliers, etc.\n\nQuestion 3\nUse poLCA to estimate the LCA with 1, 2, 3, 4, 5, and 6 classes on your small dataset. (hint: it is always a good idea to start off with a small number of indicators to check that things are going OK, before increasing the model size)\n\na. Check: Did you specify everything correctly?\nb. Do a sanity check on the results. Pay attention to: i. Reported sample sizes ii. Unexpected direction of associations iii. Forgotten indicators, or inadvertently included covariates as indicators\nc. Rerun your analysis with multiple random starts, and compare the best log-likelihood, ensuring your solution did not end up in a local maximum\nd. Model evaluation:\ni. Compare loglikelihood, BIC and AIC among the 6 models. Use a scree plot to select the number of classes.\nii. Look at bivariate residuals (BVRs). Are there any local dependencies? Is it better to increase the number of classes, or fit a model with fewer classes but local dependence?\niii. (BONUS) Perform a parametric bootstrap-likelihood ration test of your selected model. (hint: package flexmix allows parametric bootstrapping of the LRT via the function LR_test.)\ne. Model interpretation:\ni. Look at probability profiles. Without looking at the Beller (2021) paper, create a description for yourself of the profiles you have created.\nii. Create a table of the estimated class sizes. Are any classes too small to be of interest?\niii. Create a classification table and calculate the entropy \\(R^2\\). Which classes are well-separated?\niv. Look at the results for the prediction of class membership from covariates. Are the effects in the expected direction? What are the confidence intervals of the parameter estimates?\n(BONUS:) Create a plot of each covariate versus the probability to belong to each class.\n\nQuestion 4\nRepeat the analysis and interpretation steps with a larger model, using all countries.\nQuestion 5\nCompare the results from your two datasets from part 2(c) above.\nQuestion 6\nHow does your analysis compare to the results in Beller (2021)? Go through each of the steps in the previous question and report any similarities/differences.\nQuestion 7\nWhat do you conclude about profiles of health status in the ESS?\nQuestion 8\nBONUS: Bootstrap (empirically) the standard errors of the model and compare with the standard se’s given in poCLA (hint: packages flexmix and BayesLCA allow bootstrapping. To get bootstrap se’s using blca, use argument method = “em”, and blca.boot\nQuestion 9\nBONUS: Do the analysis with a newer ESS dataset and compare the results."
  },
  {
    "objectID": "Extra/Day_2/Exercise_1/5_Exercise.html",
    "href": "Extra/Day_2/Exercise_1/5_Exercise.html",
    "title": "Exercise: Attitudes towards climate change in Europe",
    "section": "",
    "text": "You can use code snippets from the earlier labs to help answer the questions of this exercise. In addition, there is some example code on the bottom of this document that could help you as well.\nQuestion 1\nRead in the ESS round 10 (2020) climate change attitudes data.\nAn easy to read codebook copied from ESS is here: https://daob.nl/files/lca/ESS10-codebook.html. The full documentation is here: https://ess-search.nsd.no/en/study/172ac431-2a06-41df-9dab-c1fd8f3877e7.\n\n\nccnthum - Climate change caused by natural processes, human activity, or both\n\nccrdprs - To what extent feel personal responsibility to reduce climate change\n\nwrclmch - How worried about climate change\n\ntestic37 - Imagine large numbers of people limit energy use, how likely reduce climate change\n\ntestic38 - How likely, large numbers of people limit energy use\n\ntestic39 - How likely, governments in enough countries take action to reduce climate change\n\ngndr - Gender\n\nagea - Age of respondent, calculated\n\neisced - Highest level of education, ES - ISCED\n\n\nNote: The data have been preprocessed by ruthlessly subjecting them to na.omit. I have also recoded eisced to be missing except for values 1-7. Otherwise, the data are as-is from the ESS website.\nQuestion 2\nIn order not to spend most of your precious time waiting, filter the data to only include one country of your choice.\nQuestion 3\nPerform any exploratory data analyses you find necessary.\nQuestion 4\nUsing poLCA, fit LCA models in which the seven participation items are used as indicators (so, exclude agea, gndr, and eisced from the analysis for now). Try models with a different number of classes. Advice: try 1–6.\nQuestion 5\nUse appropriate global fit measures, or any other criteria you prefer, to select the number of classes. Explain your choice.\nQuestion 6\nLook at local fit measures to assess the fit of your selected model.\nQuestion 7\nCreate a profile plot for your selected model. (Hint: You can use the adjusted plotting code below.)\nQuestion 8\nInterpret your selected model by looking at the profiles. How would you label the classes?\nQuestion 9\nCreate a classification table.\nQuestion 10\nCalculate the classification error and entropy \\(R^2\\).\nQuestion 11\nRefit your selected model, now while predicting class membership from agea, the square of agea, gndr, and eisced.\nQuestion 12\nPlot the probability of each class as a function of agea, gndr, and eisced, according to your model. What do you conclude?\nQuestion 13\nBONUS: Investigate the distribution of classes over countries by redoing the analyses using all countries in the ess dataset\nQuestion 14\nBONUS: Deal more appropriately with missing data, for example by using mice. You will need the original data from ESS.\n\nUseful libraries\n\nset.seed(202303)\n\nlibrary(tidyverse)\nlibrary(broom) \nlibrary(haven)\nlibrary(poLCA)\n\nRead the data from the European Social Survey, round 10 (2020).\n\ness10_climate &lt;- read_csv(\"https://daob.nl/files/lca/ess10_climate.csv.gz\") \n\ness10_climate |&gt; rmarkdown::paged_table()\n\n\n  \n\n\n\nCode to create a subset and fit a model\n\ness10_climate_it &lt;- filter(ess10_climate, cntry == \"IT\")\ness10_climate_it$ccrdprs &lt;- ess10_climate_it$ccrdprs + 1\n\nfit &lt;- poLCA(cbind(ccnthum, ccrdprs , wrclmch , \n                     testic37, testic38, testic39) ~ \n                 agea + I(agea^2) + gndr + eisced, \n                data = ess10_climate_it, nclass = 3, \n                maxiter = 2e3, nrep=10, verbose = FALSE)\n\nCode to create the profile plot (note that the assignment here differs from the lab)\n\ntidy(fit) %&gt;% # from `broom` package\n    mutate(class = as.factor(class), outcome = as.factor(outcome)) %&gt;%\n    ggplot(aes(outcome, estimate, group = class, color = class)) +\n    geom_point() + geom_line() + facet_wrap(~variable, scales = \"free_x\")+\n    geom_errorbar(aes(ymin = estimate - 2*std.error, \n                      ymax = estimate + 2*std.error), width = 0.2) +\n    theme_bw() + scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nUnfortunately, effects does not appear to function properly for this type of model. The code below could be helpful to create effects plots by hand. It assumes that the right-hand side of formula used was agea + I(agea^2) + gndr + eisced.\nThe code below creates a dataframe with the “effects” of the various covariates based on the model estimates from fit. This is also how effects works and demonstrated within the poLCA help file.\n\n# Extract posterior probabilities of each class\nposterior_df &lt;- as.data.frame(fit$posterior)\n\n# Add covariates to the dataset\nposterior_df &lt;- ess10_climate_it %&gt;%\n  dplyr::select(agea) %&gt;%\n  bind_cols(posterior_df)\n\n# Rename class probability columns\ncolnames(posterior_df)[2:ncol(posterior_df)] &lt;- paste0(\"Class_\", 1:3)\n\n# Reshape data to long format for ggplot\nposterior_long &lt;- posterior_df %&gt;%\n  pivot_longer(cols = starts_with(\"Class_\"), \n               names_to = \"Class\", values_to = \"Probability\")\n\n# Plot probability of class membership as a function of age\nggplot(posterior_long, aes(x = agea, y = Probability, color = Class)) +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  labs(title = \"Effect of Age on Class Membership\",\n       x = \"Age\", y = \"Probability of Class Membership\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Extra/Day_2/Exercise_1/5_Exercise.html#exercise-attitudes-towards-climate-change-in-europe",
    "href": "Extra/Day_2/Exercise_1/5_Exercise.html#exercise-attitudes-towards-climate-change-in-europe",
    "title": "Exercise: Attitudes towards climate change in Europe",
    "section": "",
    "text": "You can use code snippets from the earlier labs to help answer the questions of this exercise. In addition, there is some example code on the bottom of this document that could help you as well.\nQuestion 1\nRead in the ESS round 10 (2020) climate change attitudes data.\nAn easy to read codebook copied from ESS is here: https://daob.nl/files/lca/ESS10-codebook.html. The full documentation is here: https://ess-search.nsd.no/en/study/172ac431-2a06-41df-9dab-c1fd8f3877e7.\n\n\nccnthum - Climate change caused by natural processes, human activity, or both\n\nccrdprs - To what extent feel personal responsibility to reduce climate change\n\nwrclmch - How worried about climate change\n\ntestic37 - Imagine large numbers of people limit energy use, how likely reduce climate change\n\ntestic38 - How likely, large numbers of people limit energy use\n\ntestic39 - How likely, governments in enough countries take action to reduce climate change\n\ngndr - Gender\n\nagea - Age of respondent, calculated\n\neisced - Highest level of education, ES - ISCED\n\n\nNote: The data have been preprocessed by ruthlessly subjecting them to na.omit. I have also recoded eisced to be missing except for values 1-7. Otherwise, the data are as-is from the ESS website.\nQuestion 2\nIn order not to spend most of your precious time waiting, filter the data to only include one country of your choice.\nQuestion 3\nPerform any exploratory data analyses you find necessary.\nQuestion 4\nUsing poLCA, fit LCA models in which the seven participation items are used as indicators (so, exclude agea, gndr, and eisced from the analysis for now). Try models with a different number of classes. Advice: try 1–6.\nQuestion 5\nUse appropriate global fit measures, or any other criteria you prefer, to select the number of classes. Explain your choice.\nQuestion 6\nLook at local fit measures to assess the fit of your selected model.\nQuestion 7\nCreate a profile plot for your selected model. (Hint: You can use the adjusted plotting code below.)\nQuestion 8\nInterpret your selected model by looking at the profiles. How would you label the classes?\nQuestion 9\nCreate a classification table.\nQuestion 10\nCalculate the classification error and entropy \\(R^2\\).\nQuestion 11\nRefit your selected model, now while predicting class membership from agea, the square of agea, gndr, and eisced.\nQuestion 12\nPlot the probability of each class as a function of agea, gndr, and eisced, according to your model. What do you conclude?\nQuestion 13\nBONUS: Investigate the distribution of classes over countries by redoing the analyses using all countries in the ess dataset\nQuestion 14\nBONUS: Deal more appropriately with missing data, for example by using mice. You will need the original data from ESS.\n\nUseful libraries\n\nset.seed(202303)\n\nlibrary(tidyverse)\nlibrary(broom) \nlibrary(haven)\nlibrary(poLCA)\n\nRead the data from the European Social Survey, round 10 (2020).\n\ness10_climate &lt;- read_csv(\"https://daob.nl/files/lca/ess10_climate.csv.gz\") \n\ness10_climate |&gt; rmarkdown::paged_table()\n\n\n  \n\n\n\nCode to create a subset and fit a model\n\ness10_climate_it &lt;- filter(ess10_climate, cntry == \"IT\")\ness10_climate_it$ccrdprs &lt;- ess10_climate_it$ccrdprs + 1\n\nfit &lt;- poLCA(cbind(ccnthum, ccrdprs , wrclmch , \n                     testic37, testic38, testic39) ~ \n                 agea + I(agea^2) + gndr + eisced, \n                data = ess10_climate_it, nclass = 3, \n                maxiter = 2e3, nrep=10, verbose = FALSE)\n\nCode to create the profile plot (note that the assignment here differs from the lab)\n\ntidy(fit) %&gt;% # from `broom` package\n    mutate(class = as.factor(class), outcome = as.factor(outcome)) %&gt;%\n    ggplot(aes(outcome, estimate, group = class, color = class)) +\n    geom_point() + geom_line() + facet_wrap(~variable, scales = \"free_x\")+\n    geom_errorbar(aes(ymin = estimate - 2*std.error, \n                      ymax = estimate + 2*std.error), width = 0.2) +\n    theme_bw() + scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nUnfortunately, effects does not appear to function properly for this type of model. The code below could be helpful to create effects plots by hand. It assumes that the right-hand side of formula used was agea + I(agea^2) + gndr + eisced.\nThe code below creates a dataframe with the “effects” of the various covariates based on the model estimates from fit. This is also how effects works and demonstrated within the poLCA help file.\n\n# Extract posterior probabilities of each class\nposterior_df &lt;- as.data.frame(fit$posterior)\n\n# Add covariates to the dataset\nposterior_df &lt;- ess10_climate_it %&gt;%\n  dplyr::select(agea) %&gt;%\n  bind_cols(posterior_df)\n\n# Rename class probability columns\ncolnames(posterior_df)[2:ncol(posterior_df)] &lt;- paste0(\"Class_\", 1:3)\n\n# Reshape data to long format for ggplot\nposterior_long &lt;- posterior_df %&gt;%\n  pivot_longer(cols = starts_with(\"Class_\"), \n               names_to = \"Class\", values_to = \"Probability\")\n\n# Plot probability of class membership as a function of age\nggplot(posterior_long, aes(x = agea, y = Probability, color = Class)) +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  labs(title = \"Effect of Age on Class Membership\",\n       x = \"Age\", y = \"Probability of Class Membership\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "index.html#day-3",
    "href": "index.html#day-3",
    "title": "Course Material Latent Class Analysis",
    "section": "Day 3",
    "text": "Day 3\n\n\n\nTopic\nLecture\nLab\nExtra material\n\n\n\n\nLatent GOLD\nLecture Latent GOLD\n\n\n\n\nLocal dependence models and multiple latent variables\nLecture dependence models and multiple LVs\n\n\n\n\nOrdinal indicators and three-step modelling\nLecture ordinal indicators and 3-step modelling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe materials for this course were originally developed in 2023 by Daniel Oberski. You can find this version of the course on his page: https://daob.github.io/latent-class-analysis/."
  }
]