[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Material Latent Class Analysis",
    "section": "",
    "text": "On this webpage, you can find all the course materials for the GESIS course “Latent Class Analysis”."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#day-1",
    "href": "index.html#day-1",
    "title": "Course Material Latent Class Analysis",
    "section": "Day 1",
    "text": "Day 1\n\n\n\nTopic\nLecture\nLab\nExtra material\n\n\n\n\nIntroduction\nLecture slides\n\n\n\n\nEM algorithm\nLecture slides\nLPA EM"
  },
  {
    "objectID": "Labs/Day_1/Lab_EM/3_Lab_EM.html",
    "href": "Labs/Day_1/Lab_EM/3_Lab_EM.html",
    "title": "EM: simple example",
    "section": "",
    "text": "Below you will find code to simulate data from a mixture of univariate Gaussians, and estimate that mixture (LPA model) using the EM algorithm. The code is intended to be easy to understand and as simple as possible, while still doing the job.\nYou can copy code into your R environment by clicking the copy icon in the top right of each code block. You can also obtain the source code for this entire document by clicking “Code” at the top-right of this page.\n\nLook through the code and make the exercises. Your instructor is of course around to help and collaboration among participants is encouraged.\nBelow the regular questions, you will find a few “BONUS” questions. These are for those among you who are looking for a serious challenge, and you will likely find the difficulty level of these questions considerably higher. Do not worry if you do not understand these BONUS questions. Their completion is not needed for an applied understanding of LCA!\nQuestion 1\n\nset.seed(201505) # To reproduce the same \"random\" numbers\n# These are actual estimated values from the NHANES study.\n# We will take these as the true means and standard deviations \ntrue_mean_men &lt;- 1.74#m\ntrue_mean_women &lt;- 1.58#m\ntrue_sd_men &lt;- 0.08#m\ntrue_sd_women &lt;- 0.07#m\n\n# Generate fake data from the mixture distribution\nn &lt;- 1000 # Sample size\ntrue_sex &lt;- rbinom(n, size = 1, prob=0.5) + 1\n# Height is normally distributed but with different means and stdevs for men and women.\nheight &lt;- rnorm(n, c(true_mean_men, true_mean_women)[true_sex], \n                c(true_sd_men, true_sd_women)[true_sex])\n\n# Look at the data\nhist(height)\n\n\n\n\n\n\n\nRead the simulation code. Do you understand the code? Can you explain in your own words what happens here? Do you have any questions?\nQuestion 2\n\nrunit &lt;- function(maxit=3, sep_start=0.2) {\n  \n  # Choose some starting values. I choose inaccurate ones on purpose here\n  guess_mean_men &lt;- mean(height) + sep_start # We need to start with differences\n  guess_mean_wom &lt;- mean(height) - sep_start\n  guess_sd_men &lt;- sd(height)\n  guess_sd_wom &lt;- sd(height)\n  \n  cat(\"Iter:\\tM:\\tF:\\tsd(M):\\tsd(F):\\t\\n---------------------------------------\\n\")\n  cat(sprintf(\"Start\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", \n              guess_mean_men, guess_mean_wom, \n              guess_sd_men, guess_sd_wom))\n  \n  for(it in 1:maxit) {\n    # Posterior probability of being a man is the estimated proportion of the \n    #    overall height of the probability curve for men+women \n    #    that is made up by the probability curve for men:\n    pman &lt;- dnorm(height, mean = guess_mean_men, sd = guess_sd_men)\n    pwom &lt;- dnorm(height, mean = guess_mean_wom, sd = guess_sd_wom)\n    \n    post &lt;- pman / (pman + pwom)\n    \n    # The means and standard deviations for the groups of men and women are\n    #    obtained simply by using the posterior probabilities as weights. \n    # E.g. somebody with posterior 0.8 of being a man is counted 80% towards\n    #    the mean of men, and 20% towards that of women.\n    guess_mean_men &lt;- weighted.mean(height, w = post)\n    guess_mean_wom &lt;- weighted.mean(height, w = 1-post)\n    \n    guess_sd_men &lt;- sqrt(weighted.mean((height - guess_mean_men)^2, w = post))\n    guess_sd_wom &lt;- sqrt(weighted.mean((height - guess_mean_wom)^2, w = 1-post))\n    \n    # Output some of the results\n    cat(sprintf(\"%d\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", it,\n                guess_mean_men, guess_mean_wom, \n                guess_sd_men, guess_sd_wom))\n  }\n  \n  return(post) # Return the posterior probability of being a man\n}\n\nRead the function runit. Do you understand all steps?\nQuestion 3\nCode understanding check:\n\na. Why is the function dnorm used? Why is it used twice?\nb. What is happening here: post &lt;- pman / (pman + pwom)? What is the intuitive explanation of this formula?\nc. Why is weighted.mean used rather than just mean?\nd. Why are the weights chosen the way they are? e. What is the function of the for loop? When will it stop? Can you think of a different stopping rule?\n\nNow run the EM algorithm using our very own runit function.\n\n## Run the model!\n\n# Use height data to run the EM algorithm that estimates the means and stdevs of\n#    interest. Some intermediate output will be written to the screen.\npost &lt;- runit(maxit=5, sep_start=0.2)\n\nIter:   M:  F:  sd(M):  sd(F):  \n---------------------------------------\nStart   1.86    1.46    0.107   0.107\n1   1.74    1.58    0.072   0.066\n2   1.74    1.58    0.072   0.066\n3   1.74    1.58    0.073   0.066\n4   1.74    1.58    0.073   0.065\n5   1.74    1.58    0.073   0.065\n\n\nQuestion 4\nModel understanding check:\n\na. How does the EM algorithm know which group refers to men and which group refers to women? (Hint: this is a trick question)\nb. What is the height of the normal distribution curve (probability density) for: i. A 1.5 meter tall man ii. A 1.8 meter tall man iii. A 1.5 meter tall woman iv. A 1.8 meter tall woman\nc. From part (b), calculate the posterior probability to belong to the “women” class for: i. A 1.5 meter tall person of unknown sex (That is, calculate \\(P(\\text{Sex}=\\text{Woman} | \\text{Height} = 1.5)\\)) ii. A 1.8 meter tall person of unknown sex (same as above).\n\nQuestion 5\nGuessing people’s sex based on their posterior probability is not perfect. We can see this by making a cross-table between the guessed class and the true class (which here we happen to know because we created that variable ourselves in the simulation). So we guess the class based on the posterior probability and then tabulate this against the true class.\n\nsex &lt;- as.factor(true_sex)\n# Guess woman if posterior probability of being a man is less than 50%:\nguess_person_sex &lt;- as.factor(post &lt; 0.5) \nlevels(sex) &lt;- levels(guess_person_sex) &lt;- c(\"Man\", \"Woman\")\n\nknitr::kable(table(guess_person_sex, true_sex))\n\n\n\n\n1\n2\n\n\n\nMan\n436\n60\n\n\nWoman\n83\n421\n\n\n\n\n\nThis table gives the probability of being classified as a man/woman, given that you truly are one:\n\ntable(guess_person_sex, true_sex) |&gt;\n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\n\n1\n2\n\n\n\nMan\n0.8401\n0.1247\n\n\nWoman\n0.1599\n0.8753\n\n\n\n\n\nThis table is sometimes called the classification table. It is a measure of separation between the classes, and plays an important role when you want to use the classifications for some subsequent analysis. In practice, it cannot be calculated because we do not have the true_sex. Instead, an estimate can be calculated using the posterior probabilities. If these are well-calibrated (correspond to the true uncertainty about class membership), then calculating the within-guess mean of the posterior should give the desired classification table.\n\ncount_guesses &lt;- tabulate(guess_person_sex)\n\ntab &lt;- rbind(\n  tapply(post, guess_person_sex, mean), \n  tapply(1 - post, guess_person_sex, mean))\n\n# The table now estimates the probability of true class given guess. \n# We first recalculate this to a simple crosstable with counts.\nt(tab * count_guesses) |&gt; knitr::kable(digits = 1)\n\n\n\nMan\n439.5\n57.4\n\n\nWoman\n60.6\n442.4\n\n\n\n\nAgain we can show the table using column proportions, to give an estimate of the chance of correct classification given true class membership.\n\nt(tab * count_guesses) |&gt; \n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\nMan\n0.8788\n0.1148\n\n\nWoman\n0.1212\n0.8852\n\n\n\n\nQuestion 6\nBy changing the values of the relevant variables below, experiment with different settings for the means and standard deviations. Attempt to create a situation in which:\n\nThe cross-table between guessed class and true class is near-perfect;\nThe cross-table between guessed class and true class is near-useless.\n\nWhat do you conclude?\nQuestion 7\nChange the sample size to the following settings and report your findings: \\(n = 20, 50, 100, 500, 1000, 10000\\). (Be sure to re-set the parameter values for the means and standard deviations to their original values).\nQuestion 8\nUsing flexmix, we can run the same model.\n\n# Do the same as above with the flexmix library:\nlibrary(flexmix)\nheight_fit_flexmix &lt;- flexmix(height ~ 1, k = 2)\nparameters(height_fit_flexmix)\n\n                    Comp.1    Comp.2\ncoef.(Intercept) 1.6619860 1.6578146\nsigma            0.1078282 0.1070786\n\n\nSometimes flexmix converges to a local optimum. To solve this problem,we use multiple random starts (nrep = 100):\n\nheight_fit_flexmix &lt;- stepFlexmix(height ~ 1, k = 2, nrep = 100)\n\n2 : * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\nparameters(height_fit_flexmix)\n\n                     Comp.1     Comp.2\ncoef.(Intercept) 1.73242753 1.56980509\nsigma            0.07715975 0.06194694\n\n\nWe can also use mclust.\n\n# Or using the mclust library\nlibrary(mclust)\nheight_fit_mclust &lt;- Mclust(height)\nsummary(height_fit_mclust, parameters = TRUE)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust E (univariate, equal variance) model with 2 components: \n\n log-likelihood    n df      BIC      ICL\n       835.7496 1000  4 1643.868 1367.668\n\nClustering table:\n  1   2 \n542 458 \n\nMixing probabilities:\n        1         2 \n0.5365927 0.4634073 \n\nMeans:\n       1        2 \n1.583406 1.748475 \n\nVariances:\n          1           2 \n0.004763713 0.004763713 \n\n\nRun the code above to confirm that we can achieve the same result by using R packages.\nQuestion 9\nBONUS: In this exercise, we completely ignored the fact that the “prior” probabilities \\(\\pi = P(\\text{Sex} = \\text{Woman})\\) and \\(1-\\pi\\), which determine the class sizes, are not really known in practice. In other words, we set \\(\\pi\\) to its true value, \\(\\pi = 0.5\\) in our implementation. In practice, \\(\\pi\\) will be a parameter to be estimated. Implement code that does this. (Hint: you will need to adjust the E-step by using Bayes rule. The M-step for \\(\\pi\\) is just pi_est = mean(post).) Check your code by setting n to a large value, changing prob=0.5 in the simulation code to some substantially different number, and checking that your estimate corresponds to the true value. 8.\nQuestion 10 (BONUS)\nThe log-likelihood for this model is \\[\n    \\ell(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2 ; y) = \\sum_{i = 1}^n \\ln \\left[\n      \\pi \\cdot \\text{Normal}(\\mu_1, \\sigma_1) +\n      (1-\\pi) \\text{Normal}(\\mu_2, \\sigma_2)\n    \\right].\n    \\] Write code that calculates this log-likelihood, loglik, at each iteration of the EM for-loop. Double-check your code against the output of flexmix (or another package that provides this output). 9.\nQuestion 11 (BONUS)\nUsing the result from (9), implement code that terminates the for loop when the absolute relative decrease in log-likelihood, abs((loglik_current - loglik_previous)/loglik_current), say, is less than a tolerance value such as 0.001."
  },
  {
    "objectID": "Labs/Day_1/Lab_EM/3_Lab_EM.html#exercises",
    "href": "Labs/Day_1/Lab_EM/3_Lab_EM.html#exercises",
    "title": "EM: simple example",
    "section": "",
    "text": "Look through the code and make the exercises. Your instructor is of course around to help and collaboration among participants is encouraged.\nBelow the regular questions, you will find a few “BONUS” questions. These are for those among you who are looking for a serious challenge, and you will likely find the difficulty level of these questions considerably higher. Do not worry if you do not understand these BONUS questions. Their completion is not needed for an applied understanding of LCA!\nQuestion 1\n\nset.seed(201505) # To reproduce the same \"random\" numbers\n# These are actual estimated values from the NHANES study.\n# We will take these as the true means and standard deviations \ntrue_mean_men &lt;- 1.74#m\ntrue_mean_women &lt;- 1.58#m\ntrue_sd_men &lt;- 0.08#m\ntrue_sd_women &lt;- 0.07#m\n\n# Generate fake data from the mixture distribution\nn &lt;- 1000 # Sample size\ntrue_sex &lt;- rbinom(n, size = 1, prob=0.5) + 1\n# Height is normally distributed but with different means and stdevs for men and women.\nheight &lt;- rnorm(n, c(true_mean_men, true_mean_women)[true_sex], \n                c(true_sd_men, true_sd_women)[true_sex])\n\n# Look at the data\nhist(height)\n\n\n\n\n\n\n\nRead the simulation code. Do you understand the code? Can you explain in your own words what happens here? Do you have any questions?\nQuestion 2\n\nrunit &lt;- function(maxit=3, sep_start=0.2) {\n  \n  # Choose some starting values. I choose inaccurate ones on purpose here\n  guess_mean_men &lt;- mean(height) + sep_start # We need to start with differences\n  guess_mean_wom &lt;- mean(height) - sep_start\n  guess_sd_men &lt;- sd(height)\n  guess_sd_wom &lt;- sd(height)\n  \n  cat(\"Iter:\\tM:\\tF:\\tsd(M):\\tsd(F):\\t\\n---------------------------------------\\n\")\n  cat(sprintf(\"Start\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", \n              guess_mean_men, guess_mean_wom, \n              guess_sd_men, guess_sd_wom))\n  \n  for(it in 1:maxit) {\n    # Posterior probability of being a man is the estimated proportion of the \n    #    overall height of the probability curve for men+women \n    #    that is made up by the probability curve for men:\n    pman &lt;- dnorm(height, mean = guess_mean_men, sd = guess_sd_men)\n    pwom &lt;- dnorm(height, mean = guess_mean_wom, sd = guess_sd_wom)\n    \n    post &lt;- pman / (pman + pwom)\n    \n    # The means and standard deviations for the groups of men and women are\n    #    obtained simply by using the posterior probabilities as weights. \n    # E.g. somebody with posterior 0.8 of being a man is counted 80% towards\n    #    the mean of men, and 20% towards that of women.\n    guess_mean_men &lt;- weighted.mean(height, w = post)\n    guess_mean_wom &lt;- weighted.mean(height, w = 1-post)\n    \n    guess_sd_men &lt;- sqrt(weighted.mean((height - guess_mean_men)^2, w = post))\n    guess_sd_wom &lt;- sqrt(weighted.mean((height - guess_mean_wom)^2, w = 1-post))\n    \n    # Output some of the results\n    cat(sprintf(\"%d\\t%1.2f\\t%1.2f\\t%1.3f\\t%1.3f\\n\", it,\n                guess_mean_men, guess_mean_wom, \n                guess_sd_men, guess_sd_wom))\n  }\n  \n  return(post) # Return the posterior probability of being a man\n}\n\nRead the function runit. Do you understand all steps?\nQuestion 3\nCode understanding check:\n\na. Why is the function dnorm used? Why is it used twice?\nb. What is happening here: post &lt;- pman / (pman + pwom)? What is the intuitive explanation of this formula?\nc. Why is weighted.mean used rather than just mean?\nd. Why are the weights chosen the way they are? e. What is the function of the for loop? When will it stop? Can you think of a different stopping rule?\n\nNow run the EM algorithm using our very own runit function.\n\n## Run the model!\n\n# Use height data to run the EM algorithm that estimates the means and stdevs of\n#    interest. Some intermediate output will be written to the screen.\npost &lt;- runit(maxit=5, sep_start=0.2)\n\nIter:   M:  F:  sd(M):  sd(F):  \n---------------------------------------\nStart   1.86    1.46    0.107   0.107\n1   1.74    1.58    0.072   0.066\n2   1.74    1.58    0.072   0.066\n3   1.74    1.58    0.073   0.066\n4   1.74    1.58    0.073   0.065\n5   1.74    1.58    0.073   0.065\n\n\nQuestion 4\nModel understanding check:\n\na. How does the EM algorithm know which group refers to men and which group refers to women? (Hint: this is a trick question)\nb. What is the height of the normal distribution curve (probability density) for: i. A 1.5 meter tall man ii. A 1.8 meter tall man iii. A 1.5 meter tall woman iv. A 1.8 meter tall woman\nc. From part (b), calculate the posterior probability to belong to the “women” class for: i. A 1.5 meter tall person of unknown sex (That is, calculate \\(P(\\text{Sex}=\\text{Woman} | \\text{Height} = 1.5)\\)) ii. A 1.8 meter tall person of unknown sex (same as above).\n\nQuestion 5\nGuessing people’s sex based on their posterior probability is not perfect. We can see this by making a cross-table between the guessed class and the true class (which here we happen to know because we created that variable ourselves in the simulation). So we guess the class based on the posterior probability and then tabulate this against the true class.\n\nsex &lt;- as.factor(true_sex)\n# Guess woman if posterior probability of being a man is less than 50%:\nguess_person_sex &lt;- as.factor(post &lt; 0.5) \nlevels(sex) &lt;- levels(guess_person_sex) &lt;- c(\"Man\", \"Woman\")\n\nknitr::kable(table(guess_person_sex, true_sex))\n\n\n\n\n1\n2\n\n\n\nMan\n436\n60\n\n\nWoman\n83\n421\n\n\n\n\n\nThis table gives the probability of being classified as a man/woman, given that you truly are one:\n\ntable(guess_person_sex, true_sex) |&gt;\n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\n\n1\n2\n\n\n\nMan\n0.8401\n0.1247\n\n\nWoman\n0.1599\n0.8753\n\n\n\n\n\nThis table is sometimes called the classification table. It is a measure of separation between the classes, and plays an important role when you want to use the classifications for some subsequent analysis. In practice, it cannot be calculated because we do not have the true_sex. Instead, an estimate can be calculated using the posterior probabilities. If these are well-calibrated (correspond to the true uncertainty about class membership), then calculating the within-guess mean of the posterior should give the desired classification table.\n\ncount_guesses &lt;- tabulate(guess_person_sex)\n\ntab &lt;- rbind(\n  tapply(post, guess_person_sex, mean), \n  tapply(1 - post, guess_person_sex, mean))\n\n# The table now estimates the probability of true class given guess. \n# We first recalculate this to a simple crosstable with counts.\nt(tab * count_guesses) |&gt; knitr::kable(digits = 1)\n\n\n\nMan\n439.5\n57.4\n\n\nWoman\n60.6\n442.4\n\n\n\n\nAgain we can show the table using column proportions, to give an estimate of the chance of correct classification given true class membership.\n\nt(tab * count_guesses) |&gt; \n  prop.table(2) |&gt;\n  knitr::kable(digits = 4)\n\n\n\nMan\n0.8788\n0.1148\n\n\nWoman\n0.1212\n0.8852\n\n\n\n\nQuestion 6\nBy changing the values of the relevant variables below, experiment with different settings for the means and standard deviations. Attempt to create a situation in which:\n\nThe cross-table between guessed class and true class is near-perfect;\nThe cross-table between guessed class and true class is near-useless.\n\nWhat do you conclude?\nQuestion 7\nChange the sample size to the following settings and report your findings: \\(n = 20, 50, 100, 500, 1000, 10000\\). (Be sure to re-set the parameter values for the means and standard deviations to their original values).\nQuestion 8\nUsing flexmix, we can run the same model.\n\n# Do the same as above with the flexmix library:\nlibrary(flexmix)\nheight_fit_flexmix &lt;- flexmix(height ~ 1, k = 2)\nparameters(height_fit_flexmix)\n\n                    Comp.1    Comp.2\ncoef.(Intercept) 1.6619860 1.6578146\nsigma            0.1078282 0.1070786\n\n\nSometimes flexmix converges to a local optimum. To solve this problem,we use multiple random starts (nrep = 100):\n\nheight_fit_flexmix &lt;- stepFlexmix(height ~ 1, k = 2, nrep = 100)\n\n2 : * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n\nparameters(height_fit_flexmix)\n\n                     Comp.1     Comp.2\ncoef.(Intercept) 1.73242753 1.56980509\nsigma            0.07715975 0.06194694\n\n\nWe can also use mclust.\n\n# Or using the mclust library\nlibrary(mclust)\nheight_fit_mclust &lt;- Mclust(height)\nsummary(height_fit_mclust, parameters = TRUE)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust E (univariate, equal variance) model with 2 components: \n\n log-likelihood    n df      BIC      ICL\n       835.7496 1000  4 1643.868 1367.668\n\nClustering table:\n  1   2 \n542 458 \n\nMixing probabilities:\n        1         2 \n0.5365927 0.4634073 \n\nMeans:\n       1        2 \n1.583406 1.748475 \n\nVariances:\n          1           2 \n0.004763713 0.004763713 \n\n\nRun the code above to confirm that we can achieve the same result by using R packages.\nQuestion 9\nBONUS: In this exercise, we completely ignored the fact that the “prior” probabilities \\(\\pi = P(\\text{Sex} = \\text{Woman})\\) and \\(1-\\pi\\), which determine the class sizes, are not really known in practice. In other words, we set \\(\\pi\\) to its true value, \\(\\pi = 0.5\\) in our implementation. In practice, \\(\\pi\\) will be a parameter to be estimated. Implement code that does this. (Hint: you will need to adjust the E-step by using Bayes rule. The M-step for \\(\\pi\\) is just pi_est = mean(post).) Check your code by setting n to a large value, changing prob=0.5 in the simulation code to some substantially different number, and checking that your estimate corresponds to the true value. 8.\nQuestion 10 (BONUS)\nThe log-likelihood for this model is \\[\n    \\ell(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2 ; y) = \\sum_{i = 1}^n \\ln \\left[\n      \\pi \\cdot \\text{Normal}(\\mu_1, \\sigma_1) +\n      (1-\\pi) \\text{Normal}(\\mu_2, \\sigma_2)\n    \\right].\n    \\] Write code that calculates this log-likelihood, loglik, at each iteration of the EM for-loop. Double-check your code against the output of flexmix (or another package that provides this output). 9.\nQuestion 11 (BONUS)\nUsing the result from (9), implement code that terminates the for loop when the absolute relative decrease in log-likelihood, abs((loglik_current - loglik_previous)/loglik_current), say, is less than a tolerance value such as 0.001."
  }
]